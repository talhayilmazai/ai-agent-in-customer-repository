---
description: "Configuration guidelines for rocks-ecosystem YAML config files (config.yml, config_metadata.yml, config_schema_tables.yml, config_files.yml). Apply when modifying configuration files, schemas, metadata, or pipeline parameters."
globs: "**/config*.yml"
alwaysApply: false
---
# Configuration Guidelines for the rocks-ecosystem

## Core Philosophy
**Logic and Data are separated.**
- Code (`.py`) defines **HOW** we process data.
- Config (`.yml`) defines **WHAT** parameters, paths, and schemas we use.
- **Rule:** Never hardcode paths, thresholds, or table names in Python.

## 1. Introduction

Configuration is a cornerstone of the `rocks-ecosystem`, providing a flexible and maintainable way to manage pipeline parameters, data sources, and schemas without hardcoding them into the source code. This approach allows for easy modification of pipeline behavior across different environments and clients.

This guide details the structure of the configuration files, how they interact with the Python codebase, and best practices for managing them.

## 2. Core Configuration Files

The configuration is primarily managed through four YAML files located in `rocks_extension/noob/`, `rocks_extension/alerts/`, `rocks_extension/opal/`,  `rocks_extension/future_viz/`,  `rocks_extension/omega_ui/`, `rocks_extension/reporting/`:

-   `config.yml`: Defines parameters for various pipeline steps and common variables.
-   `config_metadata.yml`: Acts as the data catalog, defining the location, type, and properties of all data assets.
-   `config_schema_tables.yml`: Contains schema definitions for PySpark DataFrames.
-   `config_files.yml`: Defines the order and location of configuration files to be loaded.

## 3. `config.yml`: Pipeline Parameters

The `config.yml` file is used to configure the parameters for different modules and pipeline steps.

### Structure

-   **`commons`**: A special section for defining variables that can be reused across different configurations. This includes global settings like `forecast_horizon`, `noob_home`, `operation_home`, `master_data_home`, cloud provider configurations, and data processing settings.
-   **User Defined Partition Configuration**: `user_defined_partition_config` section for custom partitioning.
-   **Centralized Partition Configuration**: `centralized_partition_config` section for centralized partition management.
-   **Module-specific sections**: Parameters are grouped by the pipeline step or module they belong to (e.g., `daily_data`, `ensemble_forecast`, `cl_weekly_long_term_forecast`).

### Example:

```yaml
# in config.yml
commons:
  forecast_horizon: 28
  chain_long_term_forecast_horizon: 84
  long_forecast_horizon: 168
  noob_home: noob
  operation_home: operation
  master_data_home: master-data
  storage_type: parquet
  cloud_provider: local
  enforce_schema_on_read: true
  enforce_schema_on_write: true

# Centralized partition configuration
centralized_partition_config:
  central_partition_enabled: false
  central_partition_columns: null
  central_partition_filter: null
  central_partition_whitelist_tables:
    - holidays
    - ignore_tables_date
    # ... more tables

# Module-specific configuration
cl_weekly_long_term_forecast:
  forecast_horizon: ${commons.chain_long_term_forecast_horizon}
  model_id: cl_weekly_long_term_forecast
  period: week
  run_type: operation
```

In this example, `cl_weekly_long_term_forecast.forecast_horizon` automatically accesses the `commons.chain_long_term_forecast_horizon` using the `${commons.chain_long_term_forecast_horizon}` syntax, but you can also override it if needed.

### Accessing Parameters in Code

Within a pipeline step class that inherits from `NoobPipelineStep` (which in turn inherits from `rocks.core.pipeline_step.BasePipelineStepConfigurable`), parameters are automatically parsed and attached to the class instance. You can access them directly via `self.config`.

For example, in the `EnsembleForecast` step, you can access `run_type` like this:

```python
# In a class inheriting from NoobPipelineStep
run_type = self.config.run_type # "operation"
io_mode = self.config.io_control_mode # "RAISE"
```

The configuration is loaded based on the class name. For a class named `EnsembleForecast`, the system looks for the `ensemble_forecast` key in `config.yml`.

## 4. `config_metadata.yml`: The Data Catalog

This file defines all the data sources and sinks the pipeline interacts with. It's the single source of truth for data locations and metadata. The `get_data_catalog_from_config` function in `noob_base.py` is responsible for parsing this file.

### Structure

The file contains a `metadata_provider` section, which holds the `metadata` for all tables. Each entry in `metadata` defines a data asset.

### Example:

```yaml
# in config_metadata.yml
noob_home: ${commons.noob_home}
operation_home: ${commons.operation_home}
master_data_home: ${commons.master_data_home}

metadata_provider:
  metadata:
    product:
      key: ${master_data_home}/products
      primary_key:
        - product_id
      schema: ${schema_config.product.schema}
      is_partial_schema: ${oc.select:schema_config.product.is_partial_schema, null}
      user_schema: ${oc.select:schema_config.product.user_schema, null}

    sales:
      key: ${operation_home}/sales
      primary_key:
        - product_id
        - store_id
        - date
      schema: ${schema_config.sales.schema}
      is_partial_schema: ${oc.select:schema_config.sales.is_partial_schema, null}
      user_schema: ${oc.select:schema_config.sales.user_schema, null}

    forecast_prediction_dynamic_historic:
      key: ${noob_home}/forecast/{run_type}/{model_id}/predictions/historic
      partition_columns:
        - forecast_start_date
      temporal_partition_columns:
        - forecast_start_date
      repartition: forecast_start_date
      primary_key:
        - forecast_start_date
        - product_id
        - store_id
      is_historic_table: true
      schema: ${schema_config.forecast_prediction_dynamic.schema}
      is_partial_schema: true
      user_schema: ${oc.select:schema_config.forecast_prediction_dynamic.user_schema, null}
```

### Key Properties:

-   `key`: The path to the data, often using variables like `${noob_home}` or `${operation_home}` which are defined elsewhere. The path can be parameterized with placeholders like `{run_type}` which are resolved at runtime.
-   `primary_key`: The primary key of the table, used for deduplication and joins.
-   `schema`: A reference to a schema defined in `config_schema_tables.yml` using `${schema_config.<table_name>.schema}` syntax.
-   `user_schema`: A reference to a user-defined schema using `${oc.select:schema_config.<table_name>.user_schema, null}` syntax.
-   `is_historic_table`: A boolean flag indicating if the table is partitioned by date (e.g., `.../historic/` vs `.../latest/`).
-   `is_partial_schema`: If `true`, it indicates that the schema only covers a subset of the table's columns.
-   `storage_type`: The file format, e.g., `csv`, `parquet` (default is parquet).
-   `partition_columns`: A list of columns used for partitioning the data, if applicable.
-   `temporal_partition_columns`: A list of columns used for partitioning and dont include any other column than temporal columns.
-   `repartition`: Number of partitions to use or list of columns to repartition by when writing data.
-   `delta_optimize_write_flag`: Flag to control Delta table optimization during writes.

## 5. `config_schema_tables.yml`: Schema Definitions

This file centralizes all PySpark schema definitions. This ensures consistency and makes schemas reusable.

### Structure

The file has a top-level `schema_config` key. Each entry under it corresponds to a table's schema.

### Example:

```yaml
# in config_schema_tables.yml
schema_config:
  product:
    schema:
      product_id:
        type: string
        nullable: false
      brand:
        type: string
        nullable: true
    is_partial_schema: false
    
  forecast_prediction_dynamic:
    schema:
      forecast_start_date:
        type: string
        nullable: false
      product_id:
        type: integer
        nullable: false
      store_id:
        type: integer
        nullable: false
      date:
        type: string
        nullable: false
      prediction:
        type: double
        nullable: true
    is_partial_schema: true
```

### Key Properties:

-   `schema`: The dictionary defining the columns, their types (`string`, `integer`, `timestamp`, etc.), and nullability.
-   `is_partial_schema`: If `true`, it means the defined schema only covers a subset of the table's columns. This is useful for when you only need to ensure certain columns have the correct type.
-   `user_schema`: Optional user-defined schema that can override or extend the base schema.

## 6. `config_files.yml`: Configuration Loading Order

This file defines the order and location of configuration files to be loaded. It specifies which configuration files should be loaded and in what order, allowing for hierarchical configuration loading with client-specific overrides.

### Structure

The file contains a `noob` section that lists configuration file paths in order of precedence.

### Example:

```yaml
# in config_files.yml
lib_root: ${oc.env:ROCKS_LIB_ROOT_PATH, "."}

noob:
  - - ${lib_root}/rocks/noob/config.yml
    - ${lib_root}/rocks/noob/config_schema_tables.yml
    - ${lib_root}/rocks/noob/config_metadata.yml
  - - ${client_config:rocks_extension/noob/config.yml}
    - ${client_config:rocks_extension/noob/config_schema_tables.yml}
    - ${client_config:rocks_extension/noob/config_metadata.yml}
```

This allows the system to first load the base library configuration and then overlay any client-specific configuration files on top.

## 7. How It All Connects: Code Interaction

The `NoobPipelineStep` class in `noob_base.py` is the integration point for all these configuration files.

1.  **Initialization**: When a `NoobPipelineStep` is instantiated, its parent class `BasePipelineStepConfigurable` loads the relevant section from `config.yml` into `self.config`.
2.  **Data Catalog**: The `get_data_catalog_from_config` function is called to parse `config_metadata.yml` and `config_schema_tables.yml`. This creates a `DataCatalog` object.
3.  **Data Access**: A `NoobDataAccess` instance is created with the `DataCatalog`. This `self.data` object is then used for all I/O operations, ensuring that all data access is governed by the configuration.

### Adding a New Configuration

**To add a new parameter to an existing step:**

1.  Open `config.yml`.
2.  Find the section for your pipeline step (e.g., `my_pipeline_step:`).
3.  Add your new parameter: `my_new_parameter: "some_value"`.
4.  Access it in your Python code with `self.my_new_parameter`.

**To add a new data table:**

1.  **Define the schema (optional but recommended):** In `config_schema_tables.yml`, add a new entry for your table's schema.
2.  **Define the metadata:** In `config_metadata.yml`, add a new entry under `metadata`. Give it a logical name (e.g., `my_new_table`), provide its `key` (path), and link to its schema using `user_schema`.
3.  **Use in code:** Read or write the table using `self.data.try_read("my_new_table")` or `self.data.write(df, "my_new_table")`.

**Note**: Dont use hardcoded table names or paths in your code. Always use the logical names defined in `config_metadata.yml`, accessed by config attributes like `self.input_table_name` or `self.output_table_name`.

## 8. Best Practices

-   **Use `commons`**: Consider using the `commons` section in `config.yml` for parameters that are reused across multiple pipeline steps. This avoids duplication and makes it easier to change values in one place.
-   **Logical Names**: Always use logical names for data assets in `config_metadata.yml`. This allows you to change the underlying paths without affecting the code.
-   **Consistent Naming**: Use consistent naming conventions for keys in `config_metadata.yml` and `config_schema_tables.yml`. This helps avoid confusion and makes it easier to find definitions.
-   **Parameterize Paths**: Use placeholders like `{run_type}` or `{dataset_id}` in the `key` of data assets in `config_metadata.yml` to make data paths dynamic. These are resolved at runtime.
-   **Keep it Clean**: Remove unused configuration parameters and data definitions.
-   **Validate**: When adding or changing configuration, double-check for typos in keys and paths. Mismatched keys between the code and the YAML files are a common source of errors.
-   **Schema Enforcement**: Define schemas for all your important datasets in `config_schema_tables.yml` to prevent data quality issues.
