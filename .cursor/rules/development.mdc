---
description: Development guidelines and coding standards for Python code - TDD requirements, code style, configuration-driven architecture, and anti-patterns
globs: "**/*.py"
alwaysApply: false
---

# DEVELOPMENT GUIDELINES
This document serves as the **Standard Operating Procedure (SOP)** for all development activities in this repository. Both Human Engineers and AI Agents must strictly adhere to these rules.

## üõ†Ô∏è Development Environment & Commands

### Build & Distribution
Commands to package the application:

```bash
# Build wheel distribution
python3 setup.py bdist_wheel

# Clean build artifacts (always run before rebuilding)
make clean-build
```

### Test Environment
```bash
# Prepare Kubernetes test environment
make prepare-test-environment

# Remove Kubernetes test environment (cleanup)
make remove-test-environment
```

## üìñ Guidelines

### Test-Driven Development(TDD) Requirement

For every new feature or bugfix:
1. Write a failing test
2. Run to confirm failure
3. Write only enough code to pass
4. Run to confirm success
5. Refactor if needed while keeping tests green

### Code Style
- Match surrounding code style, even if it differs from standard guides
- File comments: Start all code files with a 2-line comment prefixed with "ABOUTME: " explaining what the file does.
 **Example:**
  ```text
  ABOUTME: Calculates the daily replenishment quantity based on forecast and inventory.
  ABOUTME: Handles the 'safety_stock' parameter overrides.
  ```
- Names must tell what code does, not how it's implemented
- Remove AI-generated slop: unnecessary null checks, casts to any, verbose comments stating the obvious

### Configuration-Driven Architecture

#### Configuration Usage
- **Prefer config changes over code changes.** If a problem can be solved via configuration, that should be preferred over writing new code.
- Never hardcode table names or paths
- Always use logical names from `config_metadata.yml` with `self.data.try_read()` and `self.data.write()`
- Access config parameters via `self.config.<parameter_name>` in classes inheriting from `NoobPipelineStep`

#### Coordinated Schema Changes
When modifying table structure, these files must be updated together:
- `config_schema_tables.yml` - PySpark schema definition
- `config_metadata.yml` - primary_key, repartition, path settings
- The code that produces the table
- The code that consumes the table (if column names change)

#### Configuration Patterns
Common patterns for conditional features in rocks extensions:
- `BooleanType(default_value=False)` - feature flags to enable/disable behavior
- `StringType(default_value="...")` - configurable column or table names
- `ListType(StringType(), allow_none=True)` - optional filter lists

Config parameters flow from `config.yml` ‚Üí class attributes automatically when using `NoobPipelineStep`.

### Coding Standards & Anti-Patterns

#### No Defensive Checks
**NEVER add defensive checks.** Let code fail loudly when data or columns are missing. This applies to:
- `if df is None` or `if df.rdd.isEmpty()` checks after reading data - use `required=True` instead
- `if column in df.columns` checks before using a column - just use the column
- Try/except blocks that swallow errors - let them propagate
- Default values that hide misconfiguration - fail explicitly

Bad:
```python
if "event_multiplier" in df.columns:
    df = df.withColumn("result", F.col("x") * F.col("event_multiplier"))
else:
    # Fails silently or uses default, making debugging impossible later
    df = df.withColumn("result", F.col("x"))
```

Good:
```python
# If column is missing, Spark will raise AnalysisException. This is DESIRED.
df = df.withColumn("result", F.col("x") * F.col("event_multiplier"))
```

### Versioning & Compatibility

#### No Backward Compatibility Hacks
- No renaming unused variables to `_vars`
- No re-exporting types
- No "removed" comments
- Delete unused code completely