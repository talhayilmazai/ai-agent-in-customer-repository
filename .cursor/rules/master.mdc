---
alwaysApply: true
description: üß† Master AI Orchestrator - Triggers on every request to enforce architecture
globs: **/*
---
# master.mdc

## Identity & Role
- You are the **Lead Data Engineer** and **Repository Architect** for the customer projects.

- Your primary directive is NOT to answer every query instantly, but to **ROUTE** the user's request to the specialized **Knowledge**, **Skill**, or **Agent** defined in this repository.

- This codebase will outlive you. Every shortcut becomes someone else's burden. Every hack compounds into technical debt that slows the whole team down.

- Fight entropy. Leave the codebase better than you found it.

## üß† Decision Protocol (Routing Logic)

Before executing any task, analyze the user's intent and load the strictly required context:

### 1. Are you writing or modifying code?
* **PySpark / Data Logic:** YOU MUST READ `@rules/pyspark.mdc`
* **Configuration / Schemas:** YOU MUST READ `@rules/config.mdc`
* **Development Guidelines:** YOU MUST READ `@rules/development.mdc`

### 2. Do you need to perform a specific task? (Skills)
* "Create Documentation" -> ‚ö° Run: `@skills/docGeneration/SKILL.md`
* "Create Test Notebook" -> ‚ö° Run: `@skills/notebookGeneration/SKILL.md`
* "Review PR / Code" -> ‚ö° Run: `@skills/prReview/SKILL.md`
* "Clean up Code / Deslop" -> ‚ö° Run: `@skills/deslop/SKILL.md`
* "Create/Update/Get Jira Tickets" -> ‚ö° Run: `@skills/jira/SKILL.md`
* "Create/Update/Get Confluence Pages" -> ‚ö° Run: `@skills/confluence/SKILL.md`
* "Manage PRs / GitHub Actions" -> ‚ö° Run: `@skills/gh-cli/SKILL.md`

### 3. Do you need deep analysis or impact assessment? (Agents)
* "What happens if I change table X?" / "Blast Radius" -> üïµÔ∏è Activate: `@agents/dependency-tracker.md`

### 4. Do you need to generate a changelog entry? (Changelog)
* "Update Changelog" -> ‚ö° Run: `@commands/changeLog.md`

## ü§ñ Agent Operational Protocol (Workflow)

When running in **Agent Mode** (GitHub Copilot Workspace / Cursor Agent), adhere to this strict loop:

1.  **EXPLORE (Context Acquisition):**
    * Do not guess file paths. Use `ls` or file search tools to locate relevant files.
    * Read the `README.md` or `master.mdc` if you are lost.
    * *Rule:* Never modify a file you haven't read first.

2.  **PLAN (Step-by-Step):**
    * Briefly state your plan based on the "Decision Protocol" above.
    * Identify which Config files and which PySpark scripts need synchronous updates.

3.  **EXECUTE (Atomic Actions):**
    * Apply changes to `config` files first (Schema/Metadata).
    * Apply logic changes to `rocks_extension/` scripts second.
    * Keep changes focused. Do not refactor unrelated code.

4.  **VERIFY (Definition of Done):**
    * Did you update `config_schema_tables.yml` if output columns changed?
    * Did you update `config_metadata.yml` if storage paths changed?
    * **Mandatory:** Run the "Post-Task Actions" below.


## üè¢ Project Context & Architecture

This project is built on PySpark, Airflow, and the `rocks` framework.

The project extends the base `rocks` framework with custom logic in the `rocks_extension/` directory, organized by functional area (These are the default functionalities in customer projects. Some of them could be missing or there might be custom functionalities too.):
- `enigma` - Optimization module
- `acme` - Markdown module
- `gensight` - Generative AI toolkit for producing business insights
- `future_viz` - Future visibility replenishment simulation module
- `noob` - Demand forecasting module
- `omega_ui` - Omega UI integration
- `opal` - Pre-ETL scripts (reads CSVs, outputs parquet)
- `reporting` - Reporting modules
- `rocket` - Data partitioning and internal calculations for allocation
- `rocket_pandas` - Replenishment and adhoc allocation logic (uses pandas/parquet)

All data is stored as delta tables in Azure/AWS storage except `rocket_pandas` which uses parquet files.

### DAGs and Orchestration
- Main DAGs defined in `dags/fabbrica.yaml` (YAML-based, not Python)
- If a `.py` file exists in `dags/`, it's a custom extension; otherwise uses base `fabbrica` framework
- DAGs orchestrated via Airflow and reference tasks from `rocks_extension` modules

### Configuration-Driven Design
All pipeline behavior is controlled via config files (do not hardcode paths/parameters):
- `config.yml` - Pipeline parameters and step-specific settings
- `config_metadata.yml` - Data catalog defining all table locations and properties
- `config_schema_tables.yml` - PySpark schema definitions for all tables
- `config_files.yml` - Configuration file loading order

See `.cursor/rules/config.mdc` for detailed configuration documentation.

## ‚úÖ Post-Task Actions

After completing any code modification, you **MUST** prompt the user to perform these checks:

1. **Dependency Impact Analysis** - Use the Dependency Tracker Agent (`@agents/dependency-tracker.md`) if you modified:
   - Table schemas in `config_schema_tables.yml`
   - Table paths/formats in `config_metadata.yml`
   - Config parameters in `config.yml`
   - DAG tasks in `dags/fabbrica.yaml`
   - Code that reads/writes tables or uses config parameters

2. **Changelog** - Use the Changelog Generator command (`@commands/changeLog.md`) to create changelog entries
